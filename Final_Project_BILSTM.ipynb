{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFzDxjzqhDSQzf3DDXoDTG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nabin15/Project/blob/main/Final_Project_BILSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0rvMkMR508XE",
        "outputId": "bbaa4262-fa56-4649-86bd-8d1379b0ede6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.7.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Downloading scikit_learn-1.7.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.6.1\n",
            "    Uninstalling scikit-learn-1.6.1:\n",
            "      Successfully uninstalled scikit-learn-1.6.1\n",
            "Successfully installed scikit-learn-1.7.1\n",
            "Loading datasets...\n",
            "Train shape: (10003, 2)\n",
            "Test shape: (3080, 2)\n",
            "Number of unique classes: 77\n",
            "Cleaning text data...\n",
            "Number of classes: 77\n",
            "Train samples: 9002\n",
            "Validation samples: 1001\n",
            "Test samples: 3080\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive and Install Required Packages\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Install any required packages\n",
        "!pip install --upgrade scikit-learn\n",
        "\n",
        "# Imports & Reproducibility\n",
        "import os, re, unicodedata, random, string, time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from pathlib import Path\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import (Embedding, SpatialDropout1D, Bidirectional, LSTM,\n",
        "                                     GlobalMaxPooling1D, GlobalAveragePooling1D,\n",
        "                                     Dense, Dropout, Input, Concatenate, Conv1D)\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# For model size calculation\n",
        "import pickle\n",
        "import tempfile\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); tf.random.set_seed(SEED)\n",
        "\n",
        "# 1) Load CSVs\n",
        "# Adjust paths if needed:\n",
        "TRAIN_CSV = '/content/drive/MyDrive/Project_Intent/banking77_train.csv'\n",
        "TEST_CSV  = '/content/drive/MyDrive/Project_Intent/banking77_test.csv'\n",
        "\n",
        "TEXT_COL = \"text\"\n",
        "LABEL_COL = \"category\"\n",
        "\n",
        "print(\"Loading datasets...\")\n",
        "train_df = pd.read_csv(TRAIN_CSV)\n",
        "test_df  = pd.read_csv(TEST_CSV)\n",
        "\n",
        "print(f\"Train shape: {train_df.shape}\")\n",
        "print(f\"Test shape: {test_df.shape}\")\n",
        "print(f\"Number of unique classes: {train_df[LABEL_COL].nunique()}\")\n",
        "\n",
        "# 2) Minimal cleaning\n",
        "def clean_text(s: str) -> str:\n",
        "    s = unicodedata.normalize(\"NFKC\", str(s)).lower()\n",
        "    s = (s.replace(\"’\",\"'\").replace(\"‘\",\"'\").replace(\"“\",'\"').replace(\"”\",'\"')\n",
        "           .replace(\"–\",\"-\").replace(\"—\",\"-\").replace(\"…\",\"...\"))\n",
        "    s = s.replace(\"£\",\" gbp \").replace(\"€\",\" eur \").replace(\"$\",\" usd \")\n",
        "    s = re.sub(r\"(https?://\\S+|www\\.\\S+)\", \" <url> \", s)\n",
        "    s = re.sub(r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b\", \" <email> \", s)\n",
        "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
        "\n",
        "print(\"Cleaning text data...\")\n",
        "train_df[\"clean_text\"] = train_df[TEXT_COL].astype(str).apply(clean_text)\n",
        "test_df[\"clean_text\"]  = test_df[TEXT_COL].astype(str).apply(clean_text)\n",
        "\n",
        "# 3) Encode labels (fit on TRAIN only)\n",
        "le = LabelEncoder()\n",
        "y_all_train = le.fit_transform(train_df[LABEL_COL].astype(str))\n",
        "y_test      = le.transform(test_df[LABEL_COL].astype(str))\n",
        "num_classes = len(le.classes_)\n",
        "\n",
        "print(f\"Number of classes: {num_classes}\")\n",
        "\n",
        "# 4) Stratified train/val split on CLEAN texts\n",
        "x_train_texts, x_val_texts, y_train, y_val = train_test_split(\n",
        "    train_df[\"clean_text\"].tolist(),\n",
        "    y_all_train,\n",
        "    test_size=0.10,\n",
        "    stratify=y_all_train,\n",
        "    random_state=SEED\n",
        ")\n",
        "x_test_texts = test_df[\"clean_text\"].tolist()\n",
        "\n",
        "print(f\"Train samples: {len(x_train_texts)}\")\n",
        "print(f\"Validation samples: {len(x_val_texts)}\")\n",
        "print(f\"Test samples: {len(x_test_texts)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FIRST STAGE**"
      ],
      "metadata": {
        "id": "-JynG4UTEEHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5) Tokenize & Pad\n",
        "vocab_size_v1 = 8000  # Small vocabulary\n",
        "tok_v1 = Tokenizer(num_words=vocab_size_v1, oov_token=\"<OOV>\", filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')  # Basic filters\n",
        "tok_v1.fit_on_texts(x_train_texts)\n",
        "\n",
        "def to_padded_v1(texts):\n",
        "    seq = tok_v1.texts_to_sequences(texts)\n",
        "    maxlen_v1 = 25  # Short fixed length\n",
        "    return pad_sequences(seq, maxlen=maxlen_v1, padding=\"post\", truncating=\"post\"), maxlen_v1\n",
        "\n",
        "print(\"Tokenizing and padding sequences...\")\n",
        "x_train_v1, maxlen_v1 = to_padded_v1(x_train_texts)\n",
        "x_val_v1,   _         = to_padded_v1(x_val_texts)\n",
        "x_test_v1,  _         = to_padded_v1(x_test_texts)\n",
        "\n",
        "y_train_oh_v1 = to_categorical(y_train, num_classes)\n",
        "y_val_oh_v1   = to_categorical(y_val,   num_classes)\n",
        "y_test_oh_v1  = to_categorical(y_test,  num_classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmL-gSks1ZkE",
        "outputId": "ff12b2af-3135-4749-8a08-052c439ed817"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing and padding sequences...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6) Char-level preprocessing\n",
        "print(\"Preparing character-level inputs...\")\n",
        "CHARS = list(string.ascii_lowercase + string.digits + \" \")  # Limited character set\n",
        "char_to_id = {c: i+1 for i, c in enumerate(CHARS)}  # 0 = pad\n",
        "max_char_len = 40  # Shorter character sequences\n",
        "\n",
        "def texts_to_char_ids(texts, maxlen=max_char_len):\n",
        "    arr = np.zeros((len(texts), maxlen), dtype=\"int32\")\n",
        "    for i, s in enumerate(texts):\n",
        "        s = str(s).lower()\n",
        "        ids = [char_to_id.get(ch, 0) for ch in s[:maxlen]]\n",
        "        arr[i, :len(ids)] = ids\n",
        "    return arr\n",
        "\n",
        "Xtr_char = texts_to_char_ids(x_train_texts)\n",
        "Xva_char = texts_to_char_ids(x_val_texts)\n",
        "Xte_char = texts_to_char_ids(x_test_texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25QAvZEz1e22",
        "outputId": "ba257ab5-df4d-4635-d854-f51a6807c4b7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing character-level inputs...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7) Build Word+Char BiLSTM Model\n",
        "print(\"Building Word+Char BiLSTM model...\")\n",
        "\n",
        "# Word branch (smaller)\n",
        "word_in = Input(shape=(maxlen_v1,), dtype=\"int32\")\n",
        "w = Embedding(vocab_size_v1, 64, mask_zero=True)(word_in)  # Smaller embedding\n",
        "w = SpatialDropout1D(0.20)(w)\n",
        "w = Bidirectional(LSTM(32, return_sequences=True, dropout=0.15, recurrent_dropout=0.10))(w)  # Smaller LSTM\n",
        "w = GlobalMaxPooling1D()(w)\n",
        "\n",
        "# Char branch (simpler)\n",
        "char_in = Input(shape=(max_char_len,), dtype=\"int32\")\n",
        "c = Embedding(len(char_to_id)+1, 16)(char_in)  # Smaller embedding\n",
        "c = Conv1D(32, kernel_size=3, padding=\"same\", activation=\"relu\")(c)  # Smaller conv\n",
        "c = GlobalMaxPooling1D()(c)\n",
        "\n",
        "# Fuse + classifier (simpler)\n",
        "h = Concatenate()([w, c])\n",
        "h = Dropout(0.30)(h)\n",
        "h = Dense(64, activation=\"relu\")(h)  # No regularization\n",
        "h = Dropout(0.20)(h)\n",
        "out = Dense(num_classes, activation=\"softmax\")(h)  # No regularization\n",
        "\n",
        "model_char_bilstm = Model([word_in, char_in], out)\n",
        "\n",
        "# Compile model with basic settings\n",
        "model_char_bilstm.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),  # Higher LR\n",
        "    loss=\"categorical_crossentropy\",  # No label smoothing\n",
        "    metrics=[\"accuracy\"]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRATlh2s1nFM",
        "outputId": "c0bece14-2872-4c2a-e4d1-dc4a868ece00"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building Word+Char BiLSTM model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/layer.py:965: UserWarning: Layer 'global_max_pooling1d' (of type GlobalMaxPooling1D) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#8) Training with Time Measurement\n",
        "print(\"Starting training...\")\n",
        "\n",
        "callbacks = [\n",
        "    ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=1, min_lr=1e-4, verbose=1),  # Less patience\n",
        "    EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True, verbose=1),  # Less patience\n",
        "]\n",
        "\n",
        "# Measure training time\n",
        "start_time = time.time()\n",
        "\n",
        "history = model_char_bilstm.fit(\n",
        "    [x_train_v1, Xtr_char], y_train_oh_v1,\n",
        "    epochs=30,  # Fewer epochs\n",
        "    batch_size=32,  # Smaller batch\n",
        "    validation_data=([x_val_v1, Xva_char], y_val_oh_v1),\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "print(f\"Total training time: {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "j32RD8y61rZC",
        "outputId": "ffc96e3c-0b1d-4e0d-cd64-d80d2b6d490e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Epoch 1/30\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 75ms/step - accuracy: 0.0201 - loss: 4.2681 - val_accuracy: 0.1618 - val_loss: 3.4328 - learning_rate: 0.0010\n",
            "Epoch 2/30\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 72ms/step - accuracy: 0.1656 - loss: 3.2057 - val_accuracy: 0.5025 - val_loss: 1.9594 - learning_rate: 0.0010\n",
            "Epoch 3/30\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 70ms/step - accuracy: 0.3456 - loss: 2.1997 - val_accuracy: 0.6503 - val_loss: 1.4107 - learning_rate: 0.0010\n",
            "Epoch 4/30\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 67ms/step - accuracy: 0.4709 - loss: 1.7165 - val_accuracy: 0.7073 - val_loss: 1.1041 - learning_rate: 0.0010\n",
            "Epoch 5/30\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 73ms/step - accuracy: 0.5485 - loss: 1.4339 - val_accuracy: 0.7423 - val_loss: 0.9314 - learning_rate: 0.0010\n",
            "Epoch 6/30\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 73ms/step - accuracy: 0.6222 - loss: 1.2230 - val_accuracy: 0.7582 - val_loss: 0.8378 - learning_rate: 0.0010\n",
            "Epoch 7/30\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 90ms/step - accuracy: 0.6635 - loss: 1.0971 - val_accuracy: 0.7832 - val_loss: 0.7603 - learning_rate: 0.0010\n",
            "Epoch 8/30\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 68ms/step - accuracy: 0.6915 - loss: 0.9782 - val_accuracy: 0.8012 - val_loss: 0.7086 - learning_rate: 0.0010\n",
            "Epoch 9/30\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 70ms/step - accuracy: 0.7253 - loss: 0.8905 - val_accuracy: 0.8162 - val_loss: 0.6825 - learning_rate: 0.0010\n",
            "Epoch 10/30\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 72ms/step - accuracy: 0.7419 - loss: 0.8347 - val_accuracy: 0.8212 - val_loss: 0.6603 - learning_rate: 0.0010\n",
            "Epoch 11/30\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 73ms/step - accuracy: 0.7670 - loss: 0.7810 - val_accuracy: 0.8382 - val_loss: 0.6352 - learning_rate: 0.0010\n",
            "Epoch 12/30\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 67ms/step - accuracy: 0.7699 - loss: 0.7405 - val_accuracy: 0.8352 - val_loss: 0.6235 - learning_rate: 0.0010\n",
            "Epoch 13/30\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 73ms/step - accuracy: 0.7828 - loss: 0.7109 - val_accuracy: 0.8352 - val_loss: 0.6041 - learning_rate: 0.0010\n",
            "Epoch 14/30\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 73ms/step - accuracy: 0.7967 - loss: 0.6650 - val_accuracy: 0.8462 - val_loss: 0.5806 - learning_rate: 0.0010\n",
            "Epoch 15/30\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 70ms/step - accuracy: 0.8040 - loss: 0.6465 - val_accuracy: 0.8531 - val_loss: 0.5586 - learning_rate: 0.0010\n",
            "Epoch 16/30\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.8134 - loss: 0.6200\n",
            "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 70ms/step - accuracy: 0.8134 - loss: 0.6199 - val_accuracy: 0.8541 - val_loss: 0.5755 - learning_rate: 0.0010\n",
            "Epoch 17/30\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.8216 - loss: 0.5764\n",
            "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 68ms/step - accuracy: 0.8217 - loss: 0.5762 - val_accuracy: 0.8462 - val_loss: 0.5665 - learning_rate: 5.0000e-04\n",
            "Epoch 18/30\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.8412 - loss: 0.5246\n",
            "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 73ms/step - accuracy: 0.8412 - loss: 0.5245 - val_accuracy: 0.8531 - val_loss: 0.5619 - learning_rate: 2.5000e-04\n",
            "Epoch 18: early stopping\n",
            "Restoring model weights from the end of the best epoch: 15.\n",
            "Total training time: 477.41 seconds (7.96 minutes)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SECOND STAGE**"
      ],
      "metadata": {
        "id": "VG_4795NEiJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5) Tokenize & Pad\n",
        "vocab_size_v2 = 15000  # Increased from 8K, but not yet 30K\n",
        "tok_v2 = Tokenizer(num_words=vocab_size_v2, oov_token=\"<OOV>\")\n",
        "tok_v2.fit_on_texts(x_train_texts)\n",
        "\n",
        "def to_padded_v2(texts):\n",
        "    seq = tok_v2.texts_to_sequences(texts)\n",
        "    maxlen_v2 = 35  # Increased from 25, provides more context\n",
        "    return pad_sequences(seq, maxlen=maxlen_v2, padding=\"post\", truncating=\"post\"), maxlen_v2\n",
        "\n",
        "print(\"Tokenizing and padding sequences...\")\n",
        "x_train_v2, maxlen_v2 = to_padded_v2(x_train_texts)\n",
        "x_val_v2,   _         = to_padded_v2(x_val_texts)\n",
        "x_test_v2,  _         = to_padded_v2(x_test_texts)\n",
        "\n",
        "y_train_oh_v2 = to_categorical(y_train, num_classes)\n",
        "y_val_oh_v2   = to_categorical(y_val,   num_classes)\n",
        "y_test_oh_v2  = to_categorical(y_test,  num_classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDcLF7z85-Gi",
        "outputId": "8c2a0575-ecb2-44f4-b57b-0521e0fe7d6e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing and padding sequences...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6) Char-level preprocessing\n",
        "print(\"Preparing character-level inputs...\")\n",
        "CHARS = list(string.ascii_lowercase + string.digits + \" .,'!?-\")  # Added basic punctuation\n",
        "char_to_id = {c: i+1 for i, c in enumerate(CHARS)}\n",
        "max_char_len = 50  # Increased from 40\n",
        "\n",
        "def texts_to_char_ids(texts, maxlen=max_char_len):\n",
        "    arr = np.zeros((len(texts), maxlen), dtype=\"int32\")\n",
        "    for i, s in enumerate(texts):\n",
        "        s = str(s).lower()\n",
        "        ids = [char_to_id.get(ch, 0) for ch in s[:maxlen]]\n",
        "        arr[i, :len(ids)] = ids\n",
        "    return arr\n",
        "\n",
        "Xtr_char = texts_to_char_ids(x_train_texts)\n",
        "Xva_char = texts_to_char_ids(x_val_texts)\n",
        "Xte_char = texts_to_char_ids(x_test_texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEoMrUWt6BZm",
        "outputId": "cb1c2e73-35ba-44ae-f85e-ccfd8b18efa8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing character-level inputs...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7) Build Word+Char BiLSTM Model\n",
        "print(\"Building Word+Char BiLSTM model...\")\n",
        "\n",
        "# Word branch\n",
        "word_in = Input(shape=(maxlen_v2,), dtype=\"int32\")\n",
        "w = Embedding(vocab_size_v2, 96, mask_zero=True)(word_in)  # Increased from 64\n",
        "w = SpatialDropout1D(0.25)(w)  # Slightly increased dropout\n",
        "w = Bidirectional(LSTM(48, return_sequences=True, dropout=0.18, recurrent_dropout=0.12))(w)  # Increased units\n",
        "w = GlobalMaxPooling1D()(w)\n",
        "\n",
        "# Char branch\n",
        "char_in = Input(shape=(max_char_len,), dtype=\"int32\")\n",
        "c = Embedding(len(char_to_id)+1, 24)(char_in)  # Increased from 16\n",
        "c = Conv1D(48, kernel_size=3, padding=\"same\", activation=\"relu\")(c)  # Increased filters\n",
        "c = GlobalMaxPooling1D()(c)\n",
        "\n",
        "# Fuse + classifier\n",
        "h = Concatenate()([w, c])\n",
        "h = Dropout(0.35)(h)\n",
        "h = Dense(96, activation=\"relu\")(h)  # Increased size\n",
        "h = Dropout(0.25)(h)\n",
        "out = Dense(num_classes, activation=\"softmax\")(h)\n",
        "\n",
        "model_char_bilstm = Model([word_in, char_in], out)\n",
        "\n",
        "# Compile model\n",
        "model_char_bilstm.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=7e-4),  # LR changed\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRiH1m0S6FC2",
        "outputId": "ba0380a1-57bc-4494-c51b-540ca33c66ec"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building Word+Char BiLSTM model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/layer.py:965: UserWarning: Layer 'global_max_pooling1d_4' (of type GlobalMaxPooling1D) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8) Training with Time Measurement\n",
        "print(\"Starting training...\")\n",
        "\n",
        "callbacks = [\n",
        "    ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, min_lr=1e-5, verbose=1),  # More patience\n",
        "    EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True, verbose=1),  # More patience\n",
        "]\n",
        "\n",
        "# Measure training time\n",
        "start_time = time.time()\n",
        "\n",
        "history = model_char_bilstm.fit(\n",
        "    [x_train_v2, Xtr_char], y_train_oh_v2,\n",
        "    epochs=45,  # More epochs than Stage1\n",
        "    batch_size=48,  # Batch size increased\n",
        "    validation_data=([x_val_v2, Xva_char], y_val_oh_v2),\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "print(f\"Total training time: {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "TXJEARhS6QMh",
        "outputId": "b2fb2391-c94e-4c1c-8a4d-4163a614a6ca"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Epoch 1/45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/layer.py:965: UserWarning: Layer 'global_max_pooling1d_4' (of type GlobalMaxPooling1D) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 187ms/step - accuracy: 0.0202 - loss: 4.3170 - val_accuracy: 0.0669 - val_loss: 3.9191 - learning_rate: 7.0000e-04\n",
            "Epoch 2/45\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 174ms/step - accuracy: 0.0866 - loss: 3.7152 - val_accuracy: 0.3247 - val_loss: 2.6716 - learning_rate: 7.0000e-04\n",
            "Epoch 3/45\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 154ms/step - accuracy: 0.2278 - loss: 2.7552 - val_accuracy: 0.5265 - val_loss: 1.8792 - learning_rate: 7.0000e-04\n",
            "Epoch 4/45\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 157ms/step - accuracy: 0.3674 - loss: 2.1137 - val_accuracy: 0.6274 - val_loss: 1.4407 - learning_rate: 7.0000e-04\n",
            "Epoch 5/45\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 142ms/step - accuracy: 0.4673 - loss: 1.7468 - val_accuracy: 0.6853 - val_loss: 1.1792 - learning_rate: 7.0000e-04\n",
            "Epoch 6/45\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 150ms/step - accuracy: 0.5417 - loss: 1.4844 - val_accuracy: 0.7353 - val_loss: 1.0071 - learning_rate: 7.0000e-04\n",
            "Epoch 7/45\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 132ms/step - accuracy: 0.6084 - loss: 1.2815 - val_accuracy: 0.7592 - val_loss: 0.9043 - learning_rate: 7.0000e-04\n",
            "Epoch 8/45\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 170ms/step - accuracy: 0.6397 - loss: 1.1536 - val_accuracy: 0.7842 - val_loss: 0.8069 - learning_rate: 7.0000e-04\n",
            "Epoch 9/45\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 132ms/step - accuracy: 0.6925 - loss: 1.0174 - val_accuracy: 0.7962 - val_loss: 0.7702 - learning_rate: 7.0000e-04\n",
            "Epoch 10/45\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 127ms/step - accuracy: 0.7165 - loss: 0.9365 - val_accuracy: 0.8092 - val_loss: 0.7176 - learning_rate: 7.0000e-04\n",
            "Epoch 11/45\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 133ms/step - accuracy: 0.7418 - loss: 0.8482 - val_accuracy: 0.8142 - val_loss: 0.6746 - learning_rate: 7.0000e-04\n",
            "Epoch 12/45\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 126ms/step - accuracy: 0.7586 - loss: 0.7926 - val_accuracy: 0.8302 - val_loss: 0.6536 - learning_rate: 7.0000e-04\n",
            "Epoch 13/45\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 129ms/step - accuracy: 0.7686 - loss: 0.7558 - val_accuracy: 0.8272 - val_loss: 0.6288 - learning_rate: 7.0000e-04\n",
            "Epoch 14/45\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 136ms/step - accuracy: 0.7858 - loss: 0.7036 - val_accuracy: 0.8202 - val_loss: 0.6223 - learning_rate: 7.0000e-04\n",
            "Epoch 15/45\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 120ms/step - accuracy: 0.7980 - loss: 0.6566 - val_accuracy: 0.8382 - val_loss: 0.6056 - learning_rate: 7.0000e-04\n",
            "Epoch 16/45\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 122ms/step - accuracy: 0.8036 - loss: 0.6417 - val_accuracy: 0.8382 - val_loss: 0.5995 - learning_rate: 7.0000e-04\n",
            "Epoch 17/45\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 136ms/step - accuracy: 0.8156 - loss: 0.5977 - val_accuracy: 0.8472 - val_loss: 0.5886 - learning_rate: 7.0000e-04\n",
            "Epoch 18/45\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 138ms/step - accuracy: 0.8242 - loss: 0.5661 - val_accuracy: 0.8402 - val_loss: 0.5791 - learning_rate: 7.0000e-04\n",
            "Epoch 19/45\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 122ms/step - accuracy: 0.8324 - loss: 0.5476 - val_accuracy: 0.8422 - val_loss: 0.5625 - learning_rate: 7.0000e-04\n",
            "Epoch 20/45\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 129ms/step - accuracy: 0.8375 - loss: 0.5195 - val_accuracy: 0.8492 - val_loss: 0.5662 - learning_rate: 7.0000e-04\n",
            "Epoch 21/45\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 124ms/step - accuracy: 0.8426 - loss: 0.5067 - val_accuracy: 0.8551 - val_loss: 0.5593 - learning_rate: 7.0000e-04\n",
            "Epoch 22/45\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 123ms/step - accuracy: 0.8463 - loss: 0.5009 - val_accuracy: 0.8571 - val_loss: 0.5443 - learning_rate: 7.0000e-04\n",
            "Epoch 23/45\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 133ms/step - accuracy: 0.8580 - loss: 0.4731 - val_accuracy: 0.8551 - val_loss: 0.5383 - learning_rate: 7.0000e-04\n",
            "Epoch 24/45\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 144ms/step - accuracy: 0.8607 - loss: 0.4554 - val_accuracy: 0.8531 - val_loss: 0.5458 - learning_rate: 7.0000e-04\n",
            "Epoch 25/45\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8655 - loss: 0.4487\n",
            "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 130ms/step - accuracy: 0.8655 - loss: 0.4486 - val_accuracy: 0.8611 - val_loss: 0.5437 - learning_rate: 7.0000e-04\n",
            "Epoch 26/45\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 133ms/step - accuracy: 0.8682 - loss: 0.4312 - val_accuracy: 0.8611 - val_loss: 0.5307 - learning_rate: 3.5000e-04\n",
            "Epoch 27/45\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 122ms/step - accuracy: 0.8711 - loss: 0.4176 - val_accuracy: 0.8671 - val_loss: 0.5305 - learning_rate: 3.5000e-04\n",
            "Epoch 28/45\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 132ms/step - accuracy: 0.8810 - loss: 0.3949 - val_accuracy: 0.8651 - val_loss: 0.5286 - learning_rate: 3.5000e-04\n",
            "Epoch 29/45\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 136ms/step - accuracy: 0.8717 - loss: 0.4055 - val_accuracy: 0.8711 - val_loss: 0.5198 - learning_rate: 3.5000e-04\n",
            "Epoch 30/45\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 125ms/step - accuracy: 0.8933 - loss: 0.3704 - val_accuracy: 0.8721 - val_loss: 0.5182 - learning_rate: 3.5000e-04\n",
            "Epoch 31/45\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 120ms/step - accuracy: 0.8871 - loss: 0.3716 - val_accuracy: 0.8621 - val_loss: 0.5185 - learning_rate: 3.5000e-04\n",
            "Epoch 32/45\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.8909 - loss: 0.3652\n",
            "Epoch 32: ReduceLROnPlateau reducing learning rate to 0.00017499999376013875.\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 129ms/step - accuracy: 0.8909 - loss: 0.3651 - val_accuracy: 0.8631 - val_loss: 0.5251 - learning_rate: 3.5000e-04\n",
            "Epoch 33/45\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 129ms/step - accuracy: 0.8971 - loss: 0.3373 - val_accuracy: 0.8601 - val_loss: 0.5189 - learning_rate: 1.7500e-04\n",
            "Epoch 34/45\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 140ms/step - accuracy: 0.8913 - loss: 0.3528 - val_accuracy: 0.8681 - val_loss: 0.5175 - learning_rate: 1.7500e-04\n",
            "Epoch 35/45\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 122ms/step - accuracy: 0.8895 - loss: 0.3513 - val_accuracy: 0.8651 - val_loss: 0.5143 - learning_rate: 1.7500e-04\n",
            "Epoch 36/45\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 127ms/step - accuracy: 0.8905 - loss: 0.3543 - val_accuracy: 0.8641 - val_loss: 0.5149 - learning_rate: 1.7500e-04\n",
            "Epoch 37/45\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 126ms/step - accuracy: 0.8952 - loss: 0.3543 - val_accuracy: 0.8641 - val_loss: 0.5062 - learning_rate: 1.7500e-04\n",
            "Epoch 38/45\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 139ms/step - accuracy: 0.8953 - loss: 0.3354 - val_accuracy: 0.8711 - val_loss: 0.5138 - learning_rate: 1.7500e-04\n",
            "Epoch 39/45\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.8943 - loss: 0.3271\n",
            "Epoch 39: ReduceLROnPlateau reducing learning rate to 8.749999688006938e-05.\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 122ms/step - accuracy: 0.8943 - loss: 0.3271 - val_accuracy: 0.8741 - val_loss: 0.5071 - learning_rate: 1.7500e-04\n",
            "Epoch 40/45\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 133ms/step - accuracy: 0.8989 - loss: 0.3233 - val_accuracy: 0.8731 - val_loss: 0.5073 - learning_rate: 8.7500e-05\n",
            "Epoch 41/45\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.8998 - loss: 0.3270\n",
            "Epoch 41: ReduceLROnPlateau reducing learning rate to 4.374999844003469e-05.\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 132ms/step - accuracy: 0.8998 - loss: 0.3269 - val_accuracy: 0.8711 - val_loss: 0.5121 - learning_rate: 8.7500e-05\n",
            "Epoch 42/45\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 133ms/step - accuracy: 0.8986 - loss: 0.3347 - val_accuracy: 0.8701 - val_loss: 0.5087 - learning_rate: 4.3750e-05\n",
            "Epoch 42: early stopping\n",
            "Restoring model weights from the end of the best epoch: 37.\n",
            "Total training time: 1582.94 seconds (26.38 minutes)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**THIRD STAGE**"
      ],
      "metadata": {
        "id": "INKFd1PiE44U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5) Tokenize & Pad\n",
        "vocab_size_v3 = 30000\n",
        "tok_v3 = Tokenizer(num_words=vocab_size_v3, oov_token=\"<OOV>\")\n",
        "tok_v3.fit_on_texts(x_train_texts)\n",
        "\n",
        "def to_padded_v3(texts):\n",
        "    seq = tok_v3.texts_to_sequences(texts)\n",
        "    lens = np.array([len(t.split()) for t in x_train_texts])\n",
        "    maxlen_v3 = int(np.clip(np.percentile(lens, 97), 40, 60))\n",
        "    return pad_sequences(seq, maxlen=maxlen_v3, padding=\"post\", truncating=\"post\"), maxlen_v3\n",
        "\n",
        "print(\"Tokenizing and padding sequences...\")\n",
        "x_train_v3, maxlen_v3 = to_padded_v3(x_train_texts)\n",
        "x_val_v3,   _         = to_padded_v3(x_val_texts)\n",
        "x_test_v3,  _         = to_padded_v3(x_test_texts)\n",
        "\n",
        "y_train_oh_v3 = to_categorical(y_train, num_classes)\n",
        "y_val_oh_v3   = to_categorical(y_val,   num_classes)\n",
        "y_test_oh_v3  = to_categorical(y_test,  num_classes)\n",
        "\n",
        "# OOV id for word dropout\n",
        "oov_id_v3 = tok_v3.word_index.get(\"<OOV>\", 1)\n",
        "\n",
        "def word_dropout(arr, p=0.05, pad_id=0, replace_id=None, rng=None):\n",
        "    if replace_id is None:\n",
        "        replace_id = oov_id_v3\n",
        "    if rng is None:\n",
        "        rng = np.random.default_rng()\n",
        "    mask = rng.random(arr.shape) < p\n",
        "    mask &= (arr != pad_id)  # keep paddings intact\n",
        "    out = arr.copy()\n",
        "    out[mask] = replace_id\n",
        "    return out\n",
        "\n",
        "# 6) Char-level preprocessing\n",
        "print(\"Preparing character-level inputs...\")\n",
        "CHARS = list(string.ascii_lowercase + string.digits + \" .,'!?-:/@#$_&%*+()[]{}\")\n",
        "char_to_id = {c: i+1 for i, c in enumerate(CHARS)}  # 0 = pad\n",
        "max_char_len = 60\n",
        "\n",
        "def texts_to_char_ids(texts, maxlen=max_char_len):\n",
        "    arr = np.zeros((len(texts), maxlen), dtype=\"int32\")\n",
        "    for i, s in enumerate(texts):\n",
        "        s = str(s).lower()\n",
        "        ids = [char_to_id.get(ch, 0) for ch in s[:maxlen]]\n",
        "        arr[i, :len(ids)] = ids\n",
        "    return arr\n",
        "\n",
        "Xtr_char = texts_to_char_ids(x_train_texts)\n",
        "Xva_char = texts_to_char_ids(x_val_texts)\n",
        "Xte_char = texts_to_char_ids(x_test_texts)\n",
        "\n",
        "# 7) Build Word+Char BiLSTM Model\n",
        "print(\"Building Word+Char BiLSTM model...\")\n",
        "\n",
        "# Word branch\n",
        "word_in = Input(shape=(maxlen_v3,), dtype=\"int32\")\n",
        "w = Embedding(vocab_size_v3, 128, mask_zero=True)(word_in)\n",
        "w = SpatialDropout1D(0.30)(w)\n",
        "w = Bidirectional(LSTM(64, return_sequences=True, dropout=0.20, recurrent_dropout=0.15))(w)\n",
        "w = GlobalMaxPooling1D()(w)\n",
        "\n",
        "# Char branch\n",
        "char_in = Input(shape=(max_char_len,), dtype=\"int32\")\n",
        "c = Embedding(len(char_to_id)+1, 32)(char_in)\n",
        "c = Conv1D(64, kernel_size=3, padding=\"same\", activation=\"relu\")(c)\n",
        "c = GlobalMaxPooling1D()(c)\n",
        "\n",
        "# Fuse + classifier\n",
        "h = Concatenate()([w, c])\n",
        "h = Dropout(0.40)(h)\n",
        "h = Dense(128, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(2e-4))(h)\n",
        "h = Dropout(0.30)(h)\n",
        "out = Dense(num_classes, activation=\"softmax\", kernel_regularizer=tf.keras.regularizers.l2(1e-4))(h)\n",
        "\n",
        "model_char_bilstm = Model([word_in, char_in], out)\n",
        "\n",
        "# Compile model\n",
        "model_char_bilstm.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-4, clipnorm=1.0),\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1),\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# Display model summary\n",
        "model_char_bilstm.summary()\n",
        "\n",
        "# ===================== 8) Training with Time Measurement =====================\n",
        "print(\"Starting training...\")\n",
        "\n",
        "callbacks = [\n",
        "    ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, min_lr=1e-5, verbose=1),\n",
        "    EarlyStopping(monitor=\"val_loss\", patience=6, restore_best_weights=True, verbose=1),\n",
        "    ModelCheckpoint(\"best_char_bilstm.keras\", monitor=\"val_accuracy\", save_best_only=True, verbose=1),\n",
        "]\n",
        "\n",
        "# Measure training time\n",
        "start_time = time.time()\n",
        "\n",
        "history = model_char_bilstm.fit(\n",
        "    [x_train_v3, Xtr_char], y_train_oh_v3,\n",
        "    epochs=65,\n",
        "    batch_size=64,\n",
        "    validation_data=([x_val_v3, Xva_char], y_val_oh_v3),\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "print(f\"Total training time: {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "GQ1LooUu2SVs",
        "outputId": "d1957fea-f192-485b-990a-38f089d1c7f6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing and padding sequences...\n",
            "Preparing character-level inputs...\n",
            "Building Word+Char BiLSTM model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/layer.py:965: UserWarning: Layer 'global_max_pooling1d_2' (of type GlobalMaxPooling1D) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_2       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_2         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │  \u001b[38;5;34m3,840,000\u001b[0m │ input_layer_2[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_3       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ spatial_dropout1d_1 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ embedding_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│ (\u001b[38;5;33mSpatialDropout1D\u001b[0m)  │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ input_layer_2[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_3         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m32\u001b[0m)    │      \u001b[38;5;34m1,920\u001b[0m │ input_layer_3[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ bidirectional_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │     \u001b[38;5;34m98,816\u001b[0m │ spatial_dropout1… │\n",
              "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │ not_equal_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │      \u001b[38;5;34m6,208\u001b[0m │ embedding_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_max_pooling… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ bidirectional_1[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalMaxPooling1…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_max_pooling… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ conv1d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mGlobalMaxPooling1…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ global_max_pooli… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ global_max_pooli… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ concatenate_1[\u001b[38;5;34m0\u001b[0m]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m24,704\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m77\u001b[0m)        │      \u001b[38;5;34m9,933\u001b[0m │ dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_2       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_2         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,840,000</span> │ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_3       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ spatial_dropout1d_1 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout1D</span>)  │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_3         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,920</span> │ input_layer_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ bidirectional_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span> │ spatial_dropout1… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │ not_equal_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">6,208</span> │ embedding_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_max_pooling… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bidirectional_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_max_pooling… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_max_pooli… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ global_max_pooli… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">77</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,933</span> │ dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,981,581\u001b[0m (15.19 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,981,581</span> (15.19 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,981,581\u001b[0m (15.19 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,981,581</span> (15.19 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Epoch 1/65\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - accuracy: 0.0197 - loss: 4.3717\n",
            "Epoch 1: val_accuracy improved from -inf to 0.01798, saving model to best_char_bilstm.keras\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 216ms/step - accuracy: 0.0197 - loss: 4.3716 - val_accuracy: 0.0180 - val_loss: 4.2637 - learning_rate: 5.0000e-04\n",
            "Epoch 2/65\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - accuracy: 0.0351 - loss: 4.1898\n",
            "Epoch 2: val_accuracy improved from 0.01798 to 0.12887, saving model to best_char_bilstm.keras\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 207ms/step - accuracy: 0.0352 - loss: 4.1890 - val_accuracy: 0.1289 - val_loss: 3.6843 - learning_rate: 5.0000e-04\n",
            "Epoch 3/65\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 213ms/step - accuracy: 0.1260 - loss: 3.6187\n",
            "Epoch 3: val_accuracy improved from 0.12887 to 0.35964, saving model to best_char_bilstm.keras\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 220ms/step - accuracy: 0.1262 - loss: 3.6176 - val_accuracy: 0.3596 - val_loss: 2.9598 - learning_rate: 5.0000e-04\n",
            "Epoch 4/65\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - accuracy: 0.2523 - loss: 3.0810\n",
            "Epoch 4: val_accuracy improved from 0.35964 to 0.53447, saving model to best_char_bilstm.keras\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 207ms/step - accuracy: 0.2525 - loss: 3.0803 - val_accuracy: 0.5345 - val_loss: 2.5635 - learning_rate: 5.0000e-04\n",
            "Epoch 5/65\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - accuracy: 0.3469 - loss: 2.7850\n",
            "Epoch 5: val_accuracy improved from 0.53447 to 0.60939, saving model to best_char_bilstm.keras\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 210ms/step - accuracy: 0.3471 - loss: 2.7846 - val_accuracy: 0.6094 - val_loss: 2.3126 - learning_rate: 5.0000e-04\n",
            "Epoch 6/65\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step - accuracy: 0.4469 - loss: 2.5296\n",
            "Epoch 6: val_accuracy improved from 0.60939 to 0.66933, saving model to best_char_bilstm.keras\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 211ms/step - accuracy: 0.4471 - loss: 2.5292 - val_accuracy: 0.6693 - val_loss: 2.1036 - learning_rate: 5.0000e-04\n",
            "Epoch 7/65\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - accuracy: 0.5089 - loss: 2.3645\n",
            "Epoch 7: val_accuracy improved from 0.66933 to 0.70729, saving model to best_char_bilstm.keras\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 207ms/step - accuracy: 0.5091 - loss: 2.3641 - val_accuracy: 0.7073 - val_loss: 1.9723 - learning_rate: 5.0000e-04\n",
            "Epoch 8/65\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - accuracy: 0.5674 - loss: 2.2246\n",
            "Epoch 8: val_accuracy improved from 0.70729 to 0.74525, saving model to best_char_bilstm.keras\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 209ms/step - accuracy: 0.5675 - loss: 2.2243 - val_accuracy: 0.7453 - val_loss: 1.8687 - learning_rate: 5.0000e-04\n",
            "Epoch 9/65\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - accuracy: 0.6123 - loss: 2.1011\n",
            "Epoch 9: val_accuracy improved from 0.74525 to 0.77223, saving model to best_char_bilstm.keras\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 207ms/step - accuracy: 0.6125 - loss: 2.1009 - val_accuracy: 0.7722 - val_loss: 1.7842 - learning_rate: 5.0000e-04\n",
            "Epoch 10/65\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - accuracy: 0.6561 - loss: 2.0099\n",
            "Epoch 10: val_accuracy improved from 0.77223 to 0.78721, saving model to best_char_bilstm.keras\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 209ms/step - accuracy: 0.6562 - loss: 2.0098 - val_accuracy: 0.7872 - val_loss: 1.7199 - learning_rate: 5.0000e-04\n",
            "Epoch 11/65\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - accuracy: 0.6834 - loss: 1.9243\n",
            "Epoch 11: val_accuracy did not improve from 0.78721\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 206ms/step - accuracy: 0.6835 - loss: 1.9241 - val_accuracy: 0.7862 - val_loss: 1.6755 - learning_rate: 5.0000e-04\n",
            "Epoch 12/65\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step - accuracy: 0.7067 - loss: 1.8720\n",
            "Epoch 12: val_accuracy improved from 0.78721 to 0.81119, saving model to best_char_bilstm.keras\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 221ms/step - accuracy: 0.7068 - loss: 1.8718 - val_accuracy: 0.8112 - val_loss: 1.6341 - learning_rate: 5.0000e-04\n",
            "Epoch 13/65\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - accuracy: 0.7302 - loss: 1.8110\n",
            "Epoch 13: val_accuracy improved from 0.81119 to 0.81219, saving model to best_char_bilstm.keras\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 206ms/step - accuracy: 0.7303 - loss: 1.8109 - val_accuracy: 0.8122 - val_loss: 1.5931 - learning_rate: 5.0000e-04\n",
            "Epoch 14/65\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - accuracy: 0.7432 - loss: 1.7802\n",
            "Epoch 14: val_accuracy improved from 0.81219 to 0.81818, saving model to best_char_bilstm.keras\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 207ms/step - accuracy: 0.7433 - loss: 1.7800 - val_accuracy: 0.8182 - val_loss: 1.5724 - learning_rate: 5.0000e-04\n",
            "Epoch 15/65\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step - accuracy: 0.7640 - loss: 1.7311\n",
            "Epoch 15: val_accuracy improved from 0.81818 to 0.83217, saving model to best_char_bilstm.keras\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 236ms/step - accuracy: 0.7640 - loss: 1.7310 - val_accuracy: 0.8322 - val_loss: 1.5406 - learning_rate: 5.0000e-04\n",
            "Epoch 16/65\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - accuracy: 0.7773 - loss: 1.7051\n",
            "Epoch 16: val_accuracy improved from 0.83217 to 0.83616, saving model to best_char_bilstm.keras\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 209ms/step - accuracy: 0.7773 - loss: 1.7049 - val_accuracy: 0.8362 - val_loss: 1.5141 - learning_rate: 5.0000e-04\n",
            "Epoch 17/65\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step - accuracy: 0.7938 - loss: 1.6725\n",
            "Epoch 17: val_accuracy improved from 0.83616 to 0.84515, saving model to best_char_bilstm.keras\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 220ms/step - accuracy: 0.7938 - loss: 1.6724 - val_accuracy: 0.8452 - val_loss: 1.4984 - learning_rate: 5.0000e-04\n",
            "Epoch 18/65\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - accuracy: 0.8043 - loss: 1.6307\n",
            "Epoch 18: val_accuracy improved from 0.84515 to 0.84815, saving model to best_char_bilstm.keras\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 208ms/step - accuracy: 0.8043 - loss: 1.6306 - val_accuracy: 0.8482 - val_loss: 1.4827 - learning_rate: 5.0000e-04\n",
            "Epoch 19/65\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - accuracy: 0.8183 - loss: 1.6060\n",
            "Epoch 19: val_accuracy improved from 0.84815 to 0.85015, saving model to best_char_bilstm.keras\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 208ms/step - accuracy: 0.8183 - loss: 1.6059 - val_accuracy: 0.8501 - val_loss: 1.4602 - learning_rate: 5.0000e-04\n",
            "Epoch 20/65\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - accuracy: 0.8256 - loss: 1.5843\n",
            "Epoch 20: val_accuracy improved from 0.85015 to 0.85115, saving model to best_char_bilstm.keras\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 208ms/step - accuracy: 0.8256 - loss: 1.5842 - val_accuracy: 0.8511 - val_loss: 1.4477 - learning_rate: 5.0000e-04\n",
            "Epoch 21/65\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step - accuracy: 0.8321 - loss: 1.5620\n",
            "Epoch 21: val_accuracy improved from 0.85115 to 0.85415, saving model to best_char_bilstm.keras\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 222ms/step - accuracy: 0.8321 - loss: 1.5619 - val_accuracy: 0.8541 - val_loss: 1.4370 - learning_rate: 5.0000e-04\n",
            "Epoch 22/65\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step - accuracy: 0.8366 - loss: 1.5327\n",
            "Epoch 22: val_accuracy improved from 0.85415 to 0.86114, saving model to best_char_bilstm.keras\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 209ms/step - accuracy: 0.8366 - loss: 1.5326 - val_accuracy: 0.8611 - val_loss: 1.4262 - learning_rate: 5.0000e-04\n",
            "Epoch 23/65\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - accuracy: 0.8339 - loss: 1.5290\n",
            "Epoch 23: val_accuracy improved from 0.86114 to 0.86314, saving model to best_char_bilstm.keras\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 207ms/step - accuracy: 0.8340 - loss: 1.5289 - val_accuracy: 0.8631 - val_loss: 1.4113 - learning_rate: 5.0000e-04\n",
            "Epoch 24/65\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - accuracy: 0.8554 - loss: 1.5018\n",
            "Epoch 24: val_accuracy did not improve from 0.86314\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 206ms/step - accuracy: 0.8554 - loss: 1.5018 - val_accuracy: 0.8601 - val_loss: 1.4041 - learning_rate: 5.0000e-04\n",
            "Epoch 25/65\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - accuracy: 0.8579 - loss: 1.4838\n",
            "Epoch 25: val_accuracy did not improve from 0.86314\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 206ms/step - accuracy: 0.8580 - loss: 1.4838 - val_accuracy: 0.8621 - val_loss: 1.3901 - learning_rate: 5.0000e-04\n",
            "Epoch 26/65\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - accuracy: 0.8594 - loss: 1.4685\n",
            "Epoch 26: val_accuracy did not improve from 0.86314\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 210ms/step - accuracy: 0.8594 - loss: 1.4684 - val_accuracy: 0.8611 - val_loss: 1.3865 - learning_rate: 5.0000e-04\n",
            "Epoch 27/65\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step - accuracy: 0.8656 - loss: 1.4567\n",
            "Epoch 27: val_accuracy did not improve from 0.86314\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 206ms/step - accuracy: 0.8656 - loss: 1.4567 - val_accuracy: 0.8571 - val_loss: 1.3848 - learning_rate: 5.0000e-04\n",
            "Epoch 28/65\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step - accuracy: 0.8721 - loss: 1.4349\n",
            "Epoch 28: val_accuracy did not improve from 0.86314\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 224ms/step - accuracy: 0.8721 - loss: 1.4349 - val_accuracy: 0.8581 - val_loss: 1.3716 - learning_rate: 5.0000e-04\n",
            "Epoch 29/65\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step - accuracy: 0.8747 - loss: 1.4303\n",
            "Epoch 29: val_accuracy improved from 0.86314 to 0.86813, saving model to best_char_bilstm.keras\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 210ms/step - accuracy: 0.8748 - loss: 1.4303 - val_accuracy: 0.8681 - val_loss: 1.3674 - learning_rate: 5.0000e-04\n",
            "Epoch 30/65\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - accuracy: 0.8739 - loss: 1.4275\n",
            "Epoch 30: val_accuracy did not improve from 0.86813\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 208ms/step - accuracy: 0.8739 - loss: 1.4274 - val_accuracy: 0.8661 - val_loss: 1.3624 - learning_rate: 5.0000e-04\n",
            "Epoch 31/65\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - accuracy: 0.8794 - loss: 1.4083\n",
            "Epoch 31: val_accuracy did not improve from 0.86813\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 207ms/step - accuracy: 0.8794 - loss: 1.4082 - val_accuracy: 0.8681 - val_loss: 1.3486 - learning_rate: 5.0000e-04\n",
            "Epoch 32/65\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - accuracy: 0.8802 - loss: 1.4036\n",
            "Epoch 32: val_accuracy improved from 0.86813 to 0.87413, saving model to best_char_bilstm.keras\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 208ms/step - accuracy: 0.8802 - loss: 1.4035 - val_accuracy: 0.8741 - val_loss: 1.3466 - learning_rate: 5.0000e-04\n",
            "Epoch 33/65\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - accuracy: 0.8828 - loss: 1.3929\n",
            "Epoch 33: val_accuracy improved from 0.87413 to 0.87612, saving model to best_char_bilstm.keras\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 207ms/step - accuracy: 0.8828 - loss: 1.3929 - val_accuracy: 0.8761 - val_loss: 1.3494 - learning_rate: 5.0000e-04\n",
            "Epoch 34/65\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step - accuracy: 0.8823 - loss: 1.3876\n",
            "Epoch 34: val_accuracy did not improve from 0.87612\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 227ms/step - accuracy: 0.8823 - loss: 1.3875 - val_accuracy: 0.8711 - val_loss: 1.3420 - learning_rate: 5.0000e-04\n",
            "Epoch 35/65\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - accuracy: 0.8912 - loss: 1.3718\n",
            "Epoch 35: val_accuracy did not improve from 0.87612\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 207ms/step - accuracy: 0.8913 - loss: 1.3717 - val_accuracy: 0.8721 - val_loss: 1.3417 - learning_rate: 5.0000e-04\n",
            "Epoch 36/65\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step - accuracy: 0.8951 - loss: 1.3614\n",
            "Epoch 36: val_accuracy improved from 0.87612 to 0.87812, saving model to best_char_bilstm.keras\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 210ms/step - accuracy: 0.8951 - loss: 1.3614 - val_accuracy: 0.8781 - val_loss: 1.3279 - learning_rate: 5.0000e-04\n",
            "Epoch 37/65\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - accuracy: 0.8994 - loss: 1.3519\n",
            "Epoch 37: val_accuracy did not improve from 0.87812\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 208ms/step - accuracy: 0.8994 - loss: 1.3519 - val_accuracy: 0.8751 - val_loss: 1.3274 - learning_rate: 5.0000e-04\n",
            "Epoch 38/65\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step - accuracy: 0.8990 - loss: 1.3422\n",
            "Epoch 38: val_accuracy did not improve from 0.87812\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 224ms/step - accuracy: 0.8990 - loss: 1.3422 - val_accuracy: 0.8721 - val_loss: 1.3281 - learning_rate: 5.0000e-04\n",
            "Epoch 39/65\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step - accuracy: 0.8972 - loss: 1.3455\n",
            "Epoch 39: val_accuracy did not improve from 0.87812\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 207ms/step - accuracy: 0.8972 - loss: 1.3455 - val_accuracy: 0.8761 - val_loss: 1.3187 - learning_rate: 5.0000e-04\n",
            "Epoch 40/65\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - accuracy: 0.8997 - loss: 1.3366\n",
            "Epoch 40: val_accuracy did not improve from 0.87812\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 210ms/step - accuracy: 0.8997 - loss: 1.3366 - val_accuracy: 0.8741 - val_loss: 1.3140 - learning_rate: 5.0000e-04\n",
            "Epoch 41/65\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - accuracy: 0.8988 - loss: 1.3274\n",
            "Epoch 41: val_accuracy did not improve from 0.87812\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 207ms/step - accuracy: 0.8988 - loss: 1.3273 - val_accuracy: 0.8731 - val_loss: 1.3185 - learning_rate: 5.0000e-04\n",
            "Epoch 42/65\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - accuracy: 0.9040 - loss: 1.3217\n",
            "Epoch 42: val_accuracy did not improve from 0.87812\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 206ms/step - accuracy: 0.9040 - loss: 1.3217 - val_accuracy: 0.8711 - val_loss: 1.3112 - learning_rate: 5.0000e-04\n",
            "Epoch 43/65\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step - accuracy: 0.9047 - loss: 1.3177\n",
            "Epoch 43: val_accuracy improved from 0.87812 to 0.87912, saving model to best_char_bilstm.keras\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 232ms/step - accuracy: 0.9047 - loss: 1.3176 - val_accuracy: 0.8791 - val_loss: 1.3096 - learning_rate: 5.0000e-04\n",
            "Epoch 44/65\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step - accuracy: 0.9050 - loss: 1.3003\n",
            "Epoch 44: val_accuracy did not improve from 0.87912\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 210ms/step - accuracy: 0.9050 - loss: 1.3004 - val_accuracy: 0.8751 - val_loss: 1.3082 - learning_rate: 5.0000e-04\n",
            "Epoch 45/65\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - accuracy: 0.9080 - loss: 1.3136\n",
            "Epoch 45: val_accuracy did not improve from 0.87912\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 208ms/step - accuracy: 0.9080 - loss: 1.3135 - val_accuracy: 0.8711 - val_loss: 1.3070 - learning_rate: 5.0000e-04\n",
            "Epoch 46/65\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step - accuracy: 0.9110 - loss: 1.2981\n",
            "Epoch 46: val_accuracy improved from 0.87912 to 0.88012, saving model to best_char_bilstm.keras\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 212ms/step - accuracy: 0.9111 - loss: 1.2980 - val_accuracy: 0.8801 - val_loss: 1.3037 - learning_rate: 5.0000e-04\n",
            "Epoch 47/65\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step - accuracy: 0.9062 - loss: 1.3009\n",
            "Epoch 47: val_accuracy did not improve from 0.88012\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 210ms/step - accuracy: 0.9062 - loss: 1.3008 - val_accuracy: 0.8781 - val_loss: 1.3019 - learning_rate: 5.0000e-04\n",
            "Epoch 48/65\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step - accuracy: 0.9173 - loss: 1.2895\n",
            "Epoch 48: val_accuracy did not improve from 0.88012\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 224ms/step - accuracy: 0.9173 - loss: 1.2894 - val_accuracy: 0.8751 - val_loss: 1.3024 - learning_rate: 5.0000e-04\n",
            "Epoch 49/65\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step - accuracy: 0.9119 - loss: 1.2859\n",
            "Epoch 49: val_accuracy improved from 0.88012 to 0.88212, saving model to best_char_bilstm.keras\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 212ms/step - accuracy: 0.9119 - loss: 1.2858 - val_accuracy: 0.8821 - val_loss: 1.2934 - learning_rate: 5.0000e-04\n",
            "Epoch 50/65\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - accuracy: 0.9115 - loss: 1.2910\n",
            "Epoch 50: val_accuracy did not improve from 0.88212\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 208ms/step - accuracy: 0.9115 - loss: 1.2910 - val_accuracy: 0.8811 - val_loss: 1.2995 - learning_rate: 5.0000e-04\n",
            "Epoch 51/65\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - accuracy: 0.9218 - loss: 1.2690\n",
            "Epoch 51: val_accuracy did not improve from 0.88212\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 208ms/step - accuracy: 0.9218 - loss: 1.2690 - val_accuracy: 0.8771 - val_loss: 1.2920 - learning_rate: 5.0000e-04\n",
            "Epoch 52/65\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step - accuracy: 0.9209 - loss: 1.2701\n",
            "Epoch 52: val_accuracy did not improve from 0.88212\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 223ms/step - accuracy: 0.9209 - loss: 1.2701 - val_accuracy: 0.8781 - val_loss: 1.2957 - learning_rate: 5.0000e-04\n",
            "Epoch 53/65\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step - accuracy: 0.9194 - loss: 1.2640\n",
            "Epoch 53: val_accuracy did not improve from 0.88212\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 209ms/step - accuracy: 0.9195 - loss: 1.2640 - val_accuracy: 0.8771 - val_loss: 1.2904 - learning_rate: 5.0000e-04\n",
            "Epoch 54/65\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step - accuracy: 0.9236 - loss: 1.2565\n",
            "Epoch 54: val_accuracy did not improve from 0.88212\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 208ms/step - accuracy: 0.9236 - loss: 1.2565 - val_accuracy: 0.8791 - val_loss: 1.2833 - learning_rate: 5.0000e-04\n",
            "Epoch 55/65\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - accuracy: 0.9186 - loss: 1.2584\n",
            "Epoch 55: val_accuracy did not improve from 0.88212\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 207ms/step - accuracy: 0.9186 - loss: 1.2584 - val_accuracy: 0.8801 - val_loss: 1.2812 - learning_rate: 5.0000e-04\n",
            "Epoch 56/65\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 208ms/step - accuracy: 0.9274 - loss: 1.2430\n",
            "Epoch 56: val_accuracy did not improve from 0.88212\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 218ms/step - accuracy: 0.9274 - loss: 1.2430 - val_accuracy: 0.8801 - val_loss: 1.2849 - learning_rate: 5.0000e-04\n",
            "Epoch 57/65\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step - accuracy: 0.9216 - loss: 1.2588\n",
            "Epoch 57: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\n",
            "Epoch 57: val_accuracy did not improve from 0.88212\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 208ms/step - accuracy: 0.9217 - loss: 1.2587 - val_accuracy: 0.8801 - val_loss: 1.2825 - learning_rate: 5.0000e-04\n",
            "Epoch 58/65\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - accuracy: 0.9277 - loss: 1.2407\n",
            "Epoch 58: val_accuracy improved from 0.88212 to 0.88312, saving model to best_char_bilstm.keras\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 213ms/step - accuracy: 0.9278 - loss: 1.2406 - val_accuracy: 0.8831 - val_loss: 1.2751 - learning_rate: 2.5000e-04\n",
            "Epoch 59/65\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - accuracy: 0.9287 - loss: 1.2328\n",
            "Epoch 59: val_accuracy did not improve from 0.88312\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 210ms/step - accuracy: 0.9287 - loss: 1.2327 - val_accuracy: 0.8801 - val_loss: 1.2790 - learning_rate: 2.5000e-04\n",
            "Epoch 60/65\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - accuracy: 0.9309 - loss: 1.2340\n",
            "Epoch 60: val_accuracy did not improve from 0.88312\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 209ms/step - accuracy: 0.9309 - loss: 1.2339 - val_accuracy: 0.8801 - val_loss: 1.2725 - learning_rate: 2.5000e-04\n",
            "Epoch 61/65\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step - accuracy: 0.9349 - loss: 1.2252\n",
            "Epoch 61: val_accuracy improved from 0.88312 to 0.88412, saving model to best_char_bilstm.keras\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 235ms/step - accuracy: 0.9349 - loss: 1.2252 - val_accuracy: 0.8841 - val_loss: 1.2734 - learning_rate: 2.5000e-04\n",
            "Epoch 62/65\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step - accuracy: 0.9345 - loss: 1.2185\n",
            "Epoch 62: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\n",
            "Epoch 62: val_accuracy improved from 0.88412 to 0.88511, saving model to best_char_bilstm.keras\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 210ms/step - accuracy: 0.9345 - loss: 1.2185 - val_accuracy: 0.8851 - val_loss: 1.2756 - learning_rate: 2.5000e-04\n",
            "Epoch 63/65\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - accuracy: 0.9365 - loss: 1.2167\n",
            "Epoch 63: val_accuracy did not improve from 0.88511\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 208ms/step - accuracy: 0.9365 - loss: 1.2167 - val_accuracy: 0.8851 - val_loss: 1.2724 - learning_rate: 1.2500e-04\n",
            "Epoch 64/65\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step - accuracy: 0.9293 - loss: 1.2208\n",
            "Epoch 64: val_accuracy did not improve from 0.88511\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 209ms/step - accuracy: 0.9293 - loss: 1.2208 - val_accuracy: 0.8841 - val_loss: 1.2718 - learning_rate: 1.2500e-04\n",
            "Epoch 65/65\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step - accuracy: 0.9359 - loss: 1.2220\n",
            "Epoch 65: val_accuracy did not improve from 0.88511\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 222ms/step - accuracy: 0.9359 - loss: 1.2219 - val_accuracy: 0.8811 - val_loss: 1.2720 - learning_rate: 1.2500e-04\n",
            "Restoring model weights from the end of the best epoch: 64.\n",
            "Total training time: 2411.88 seconds (40.20 minutes)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9) Load Best Model and Evaluate\n",
        "print(\"Loading best model and evaluating...\")\n",
        "model_char_bilstm.load_weights(\"best_char_bilstm.keras\")\n",
        "\n",
        "# Measure inference time\n",
        "start_time = time.time()\n",
        "y_prob = model_char_bilstm.predict([x_test_v3, Xte_char], batch_size=256, verbose=0)\n",
        "inference_time = time.time() - start_time\n",
        "\n",
        "y_pred = y_prob.argmax(axis=1)\n",
        "\n",
        "# Calculate metrics\n",
        "test_acc = accuracy_score(y_test, y_pred)\n",
        "test_f1_macro = f1_score(y_test, y_pred, average=\"macro\")\n",
        "test_f1_weighted = f1_score(y_test, y_pred, average=\"weighted\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"FINAL RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "print(f\"Test Macro-F1: {test_f1_macro:.4f}\")\n",
        "print(f\"Test Weighted-F1: {test_f1_weighted:.4f}\")\n",
        "print(f\"Training Time: {training_time:.2f} seconds\")\n",
        "print(f\"Inference Time (full test set): {inference_time:.4f} seconds\")\n",
        "print(f\"Inference Time per sample: {inference_time/len(x_test_texts)*1000:.4f} ms\")\n",
        "\n",
        "# 10) Calculate Model Size\n",
        "print(\"Calculating model size...\")\n",
        "\n",
        "def get_model_size(model):\n",
        "    # Save model to temporary file and check size\n",
        "    with tempfile.NamedTemporaryFile(suffix='.h5', delete=False) as tmp:\n",
        "        model.save(tmp.name)\n",
        "        model_size = os.path.getsize(tmp.name)\n",
        "        os.unlink(tmp.name)\n",
        "    return model_size\n",
        "\n",
        "def get_model_size_pickle(model):\n",
        "    # Alternative method: pickle the model\n",
        "    with tempfile.NamedTemporaryFile(suffix='.pkl', delete=False) as tmp:\n",
        "        pickle.dump(model, tmp)\n",
        "        model_size = os.path.getsize(tmp.name)\n",
        "        os.unlink(tmp.name)\n",
        "    return model_size\n",
        "\n",
        "# Calculate model size\n",
        "model_size_bytes = get_model_size(model_char_bilstm)\n",
        "model_size_mb = model_size_bytes / (1024 * 1024)\n",
        "\n",
        "print(f\"Model size: {model_size_bytes:,} bytes ({model_size_mb:.2f} MB)\")\n",
        "\n",
        "# 11) Detailed Classification Report\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"DETAILED CLASSIFICATION REPORT\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(classification_report(y_test, y_pred, target_names=le.classes_, digits=4))\n",
        "\n",
        "# 12) Save Results to File\n",
        "results = {\n",
        "    'test_accuracy': test_acc,\n",
        "    'test_f1_macro': test_f1_macro,\n",
        "    'test_f1_weighted': test_f1_weighted,\n",
        "    'training_time_seconds': training_time,\n",
        "    'inference_time_seconds': inference_time,\n",
        "    'inference_time_per_sample_ms': inference_time/len(x_test_texts)*1000,\n",
        "    'model_size_bytes': model_size_bytes,\n",
        "    'model_size_mb': model_size_mb,\n",
        "    'num_parameters': model_char_bilstm.count_params()\n",
        "}\n",
        "\n",
        "results_df = pd.DataFrame([results])\n",
        "print(\"\\nResults summary:\")\n",
        "print(results_df.to_string())\n",
        "\n",
        "# Save results to CSV\n",
        "results_df.to_csv('word_char_bilstm_results.csv', index=False)\n",
        "print(\"Results saved to 'word_char_bilstm_results.csv'\")\n",
        "\n",
        "# 13) Plot Training History (Extra)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"Training history plot saved as 'training_history.png'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "n30dkKOp2hbt",
        "outputId": "010cc94a-e517-4fae-cdf0-cb8ec63f252f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading best model and evaluating...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "FINAL RESULTS\n",
            "============================================================\n",
            "Test Accuracy: 0.8860\n",
            "Test Macro-F1: 0.8863\n",
            "Test Weighted-F1: 0.8863\n",
            "Training Time: 2411.88 seconds\n",
            "Inference Time (full test set): 3.4207 seconds\n",
            "Inference Time per sample: 1.1106 ms\n",
            "Calculating model size...\n",
            "Model size: 47,849,936 bytes (45.63 MB)\n",
            "\n",
            "============================================================\n",
            "DETAILED CLASSIFICATION REPORT\n",
            "============================================================\n",
            "                                                  precision    recall  f1-score   support\n",
            "\n",
            "                           Refund_not_showing_up     0.8864    0.9750    0.9286        40\n",
            "                                activate_my_card     0.9750    0.9750    0.9750        40\n",
            "                                       age_limit     1.0000    0.9500    0.9744        40\n",
            "                         apple_pay_or_google_pay     1.0000    0.9750    0.9873        40\n",
            "                                     atm_support     0.9286    0.9750    0.9512        40\n",
            "                                automatic_top_up     1.0000    0.9000    0.9474        40\n",
            "         balance_not_updated_after_bank_transfer     0.7143    0.7500    0.7317        40\n",
            "balance_not_updated_after_cheque_or_cash_deposit     0.9459    0.8750    0.9091        40\n",
            "                         beneficiary_not_allowed     0.8947    0.8500    0.8718        40\n",
            "                                 cancel_transfer     0.9722    0.8750    0.9211        40\n",
            "                            card_about_to_expire     0.9750    0.9750    0.9750        40\n",
            "                                 card_acceptance     0.8500    0.8500    0.8500        40\n",
            "                                    card_arrival     0.7778    0.8750    0.8235        40\n",
            "                          card_delivery_estimate     0.8049    0.8250    0.8148        40\n",
            "                                    card_linking     0.9487    0.9250    0.9367        40\n",
            "                                card_not_working     0.7059    0.9000    0.7912        40\n",
            "                        card_payment_fee_charged     0.7955    0.8750    0.8333        40\n",
            "                     card_payment_not_recognised     0.7727    0.8500    0.8095        40\n",
            "                card_payment_wrong_exchange_rate     0.8810    0.9250    0.9024        40\n",
            "                                  card_swallowed     1.0000    0.8750    0.9333        40\n",
            "                          cash_withdrawal_charge     0.9268    0.9500    0.9383        40\n",
            "                  cash_withdrawal_not_recognised     0.8837    0.9500    0.9157        40\n",
            "                                      change_pin     0.9048    0.9500    0.9268        40\n",
            "                                compromised_card     0.9032    0.7000    0.7887        40\n",
            "                         contactless_not_working     0.9697    0.8000    0.8767        40\n",
            "                                 country_support     0.9250    0.9250    0.9250        40\n",
            "                           declined_card_payment     0.7451    0.9500    0.8352        40\n",
            "                        declined_cash_withdrawal     0.8478    0.9750    0.9070        40\n",
            "                               declined_transfer     0.9688    0.7750    0.8611        40\n",
            "             direct_debit_payment_not_recognised     0.9211    0.8750    0.8974        40\n",
            "                          disposable_card_limits     0.9189    0.8500    0.8831        40\n",
            "                           edit_personal_details     0.9500    0.9500    0.9500        40\n",
            "                                 exchange_charge     0.9737    0.9250    0.9487        40\n",
            "                                   exchange_rate     0.8810    0.9250    0.9024        40\n",
            "                                exchange_via_app     0.8085    0.9500    0.8736        40\n",
            "                       extra_charge_on_statement     0.8636    0.9500    0.9048        40\n",
            "                                 failed_transfer     0.7778    0.8750    0.8235        40\n",
            "                           fiat_currency_support     0.9429    0.8250    0.8800        40\n",
            "                     get_disposable_virtual_card     0.8333    0.8750    0.8537        40\n",
            "                               get_physical_card     0.9286    0.9750    0.9512        40\n",
            "                              getting_spare_card     0.9730    0.9000    0.9351        40\n",
            "                            getting_virtual_card     0.8636    0.9500    0.9048        40\n",
            "                             lost_or_stolen_card     0.7907    0.8500    0.8193        40\n",
            "                            lost_or_stolen_phone     1.0000    0.9500    0.9744        40\n",
            "                             order_physical_card     0.8293    0.8500    0.8395        40\n",
            "                              passcode_forgotten     0.9091    1.0000    0.9524        40\n",
            "                            pending_card_payment     0.9189    0.8500    0.8831        40\n",
            "                         pending_cash_withdrawal     0.9737    0.9250    0.9487        40\n",
            "                                  pending_top_up     0.8974    0.8750    0.8861        40\n",
            "                                pending_transfer     0.8710    0.6750    0.7606        40\n",
            "                                     pin_blocked     0.9706    0.8250    0.8919        40\n",
            "                                 receiving_money     0.9000    0.9000    0.9000        40\n",
            "                                  request_refund     0.9000    0.9000    0.9000        40\n",
            "                          reverted_card_payment?     0.8750    0.8750    0.8750        40\n",
            "                  supported_cards_and_currencies     0.8222    0.9250    0.8706        40\n",
            "                               terminate_account     0.9268    0.9500    0.9383        40\n",
            "                  top_up_by_bank_transfer_charge     1.0000    0.8000    0.8889        40\n",
            "                           top_up_by_card_charge     0.8837    0.9500    0.9157        40\n",
            "                        top_up_by_cash_or_cheque     0.8718    0.8500    0.8608        40\n",
            "                                   top_up_failed     0.8182    0.9000    0.8571        40\n",
            "                                   top_up_limits     0.9500    0.9500    0.9500        40\n",
            "                                 top_up_reverted     0.8649    0.8000    0.8312        40\n",
            "                              topping_up_by_card     0.8824    0.7500    0.8108        40\n",
            "                       transaction_charged_twice     0.8864    0.9750    0.9286        40\n",
            "                            transfer_fee_charged     0.8444    0.9500    0.8941        40\n",
            "                           transfer_into_account     0.8611    0.7750    0.8158        40\n",
            "              transfer_not_received_by_recipient     0.6667    0.8000    0.7273        40\n",
            "                                 transfer_timing     0.8049    0.8250    0.8148        40\n",
            "                       unable_to_verify_identity     0.9143    0.8000    0.8533        40\n",
            "                              verify_my_identity     0.8235    0.7000    0.7568        40\n",
            "                          verify_source_of_funds     0.9302    1.0000    0.9639        40\n",
            "                                   verify_top_up     1.0000    0.9750    0.9873        40\n",
            "                        virtual_card_not_working     0.9688    0.7750    0.8611        40\n",
            "                              visa_or_mastercard     0.9737    0.9250    0.9487        40\n",
            "                             why_verify_identity     0.7083    0.8500    0.7727        40\n",
            "                   wrong_amount_of_cash_received     0.9231    0.9000    0.9114        40\n",
            "         wrong_exchange_rate_for_cash_withdrawal     0.9714    0.8500    0.9067        40\n",
            "\n",
            "                                        accuracy                         0.8860      3080\n",
            "                                       macro avg     0.8918    0.8860    0.8863      3080\n",
            "                                    weighted avg     0.8918    0.8860    0.8863      3080\n",
            "\n",
            "\n",
            "Results summary:\n",
            "   test_accuracy  test_f1_macro  test_f1_weighted  training_time_seconds  inference_time_seconds  inference_time_per_sample_ms  model_size_bytes  model_size_mb  num_parameters\n",
            "0       0.886039       0.886322          0.886322             2411.88187                3.420713                      1.110621          47849936      45.633255         3981581\n",
            "Results saved to 'word_char_bilstm_results.csv'\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training history plot saved as 'training_history.png'\n"
          ]
        }
      ]
    }
  ]
}